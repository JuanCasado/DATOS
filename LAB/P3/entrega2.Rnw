\documentclass [a4paper] {article}
\usepackage[utf8]{inputenc}
\title{Ciencia de datos, práctica 2}
\author{Juan Casado Ballesteros, Samuel García Gonzalez, Iván Anaya Martín}
\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\newpage
\tableofcontents
\newpage

<<cargar_funciones_credas, echo=FALSE>>=
library("rpart")
library("tree")
library("rpart.plot")
library("maptree")
#-----------------------------------------------
mediaAritmetica <- function(data){
  acc <- 0
  for (value in data) {
    acc <- acc + value
  }
  acc / length(data)
}
varianza <- function(data) {
  v_media <- mediaAritmetica(data)
  acc = 0
  for (value in data){
    acc <- acc + (value - v_media)^2
  }
  acc/length(data)
}
desviacionTipica <- function (data) {
  varianza(data)^(1/2)
}
#-----------------------------------------------
covarianza <- function(x, y) {
  media_x <- mediaAritmetica(x)
  media_y <- mediaAritmetica(y)
  len <- min(length(x), length(y))
  acc <- 0
  for (i in 1:len) {
    acc <- acc + x[i] * y[i]
  }
  acc/len - media_x*media_y
}
correlacion <- function(x, y) {
  covarianza_ <- covarianza(x, y)
  desviacion_tipica_x <- desviacionTipica(x)
  desviacion_tipica_y <- desviacionTipica(y)
  covarianza_/(desviacion_tipica_x*desviacion_tipica_y)
}
SSR <- function (x, y, regresion){
  media_y <- mediaAritmetica(y)
  acc <- 0
  for (value in x){
    acc <- acc + ((regresion2$coefficients[2]*value+regresion2$coefficients[1]) - media_y)^2
  }
  acc
}
SSY <- function (y, regresion){
  media_y <- mediaAritmetica(y)
  acc <- 0
  for (value in y){
    acc <- acc + (value - media_y)^2
  }
  acc
}
correlacionCuadrada <- function (x, y ,regresion){
  SSR(x, y ,regresion) / SSY(y ,regresion)
}
errorEstandar <- function (x, y, regresion) {
  len <- min(length(x), length(y))
  acc <- 0
  for (i in 1:len){
    acc <- acc + (y[i] - (regresion2$coefficients[2]*x[i]+regresion2$coefficients[1]))^2
  }
  sqrt(acc/(len))
}
regLine <- function (x, y) {
  covarianza_ <- covarianza (x, y)
  varianza_ <- varianza (x)
  media_x <- mediaAritmetica(x)
  media_y <- mediaAritmetica(y)
  a <- covarianza_/varianza_
  b <- media_y - a * media_x
  data.frame(coefficients=c(b, a))
}
regPlot <- function (x, y, regresion) {
  plot(x, y)
  reg95up <- regresion
  reg95up$coefficients[1] = reg95up$coefficients[1] + 2*summary(regresion)$sigma
  reg66up <- regresion
  reg66up$coefficients[1] = reg66up$coefficients[1] + summary(regresion)$sigma
  reg66down <- regresion
  reg66down$coefficients[1] = reg66down$coefficients[1] - summary(regresion)$sigma
  reg95down <- regresion
  reg95down$coefficients[1] = reg95down$coefficients[1] - 2*summary(regresion)$sigma
  abline(reg95up, "gray", lty=3, lwd=1)
  abline(reg66up, "gray", lty=2, lwd=2)
  abline(regresion, "black", lty=1, lwd=3)
  abline(reg66down, "gray", lty=2, lwd=2)
  abline(reg95down, "gray", lty=3, lwd=1)
}
@

\section{EJ1}

\begin{center}
@
<<rpart1_gini, fig=TRUE>>=
muestra1 = data.frame(read.table("datos1.txt"))
clas_1gini=rpart(Global~., data=muestra1,method="class",minsplit=1)
rpart.plot(clas_1gini)
@
\end{center}

\begin{center}
@
<<rpart1_entropia, fig=TRUE>>=
clas_1entropia=rpart(Global~., data=muestra1,method="class",minsplit=1,parms=list(split="information"))
rpart.plot(clas_1entropia)
@
\end{center}

\begin{center}
@
<<tree1, fig=TRUE>>=
clas_1tree=tree(Global~.,data=muestra1,mincut=1,minsize=2) 
draw.tree(clas_1tree)
@
\end{center}

\section{EJ2}
Creamos un .txt con los datos proporcionados sobre el radio y densidad de los planetas y lo leemos.
<<cargar_datos2>>=
datos2 <- read.table("datos2.txt")
datos2
@

Calculamos la regresión sobre dichos datos para obtener la recta que más se aproxime a los puntos que tenemos.
<<regresion_datos2>>=
regresion2 <- lm(Densidad~Radio, data=datos2)
regresion2_own <- regLine(datos2$Radio, datos2$Densidad)
@

Podemos ver los valores que adopta la ecucaión de la recta que se generará.

y = ax + b

b=
<<recta_datos2>>=
regresion2$coefficients[1]
regresion2_own$coefficients[1]
@

a=
<<recta_datos2>>=
regresion2$coefficients[2]
regresion2_own$coefficients[2]
@

Cuando calculamos la recta de regresión sobre unos datos es necesario evaluar la calidad de esta.
Debemos analizar cómo de bien se ajusta a nuestros datos.
Podemos ver esta información mediante summary.

\subsection{Residuos}
Diferencias entre cada valor de y real y cada valor de y obtenido mediante la función de regresión.
<<summary_datos2>>=
summary(regresion2)$residuals
@

\subsection{Coeficientes}
Coeficientes estimados para y error estándar para cada uno de ellos.
<<summary_datos2>>=
summary(regresion2)$coefficients
@

\subsection{Error estándar}
R implemnta el error estándar de la población y no el de la muestra que el que hemos visto en clase por lo que los cálculos no coincidirán.
El error estándar de la población se divide por n-2 y el de la muestra solo por n.
Cuanto más próximo a 0 sea el error estándar mejor será la recta de regresión.
<<summary_datos2>>=
summary(regresion2)$sigma
errorEstandar(datos2$Radio, datos2$Densidad, regresion2)
@

\subsection{Correlación cuadrada}
Podemos comprobar que coincide con nuestra implementación.
Este valor está entre 0 y 1 siendo mejor cuanto más próximo a 1 sea (idealmente a partir de 0.8).
<<summary_datos2>>=
summary(regresion2)$r.squared
correlacionCuadrada(datos2$Radio, datos2$Densidad)
@

Para finalizar dibujaremos una gráfica en la que se representarán lod datos junto a la recta de regresión.
Paralela a la recta de regresión dibujaremos las rectas que marcan el error estándar entorno a la recta de regresión.
En trazado gris grueso la que marca la región en la que estarán el 66\% de los datos y en gris fino la que marca el 95\%.

<<plot_regresion2, fig=TRUE, echo=FALSE>>=
regPlot (datos2$Radio, datos2$Densidad, regresion2)
@

Como podemos ver la recta se ajusta muy mal a los datos que tenemos quedando las rectas que marcan el error estandas fuera del gráfico.
La correlación cuadrada obtenida es muy baja.
En parte esto se debe a que tenemos muy pocos datos.


\section{EJ3}

En esta parte realizamos el algoritmo Hunt con la librería rpart sobre los datos de los vehículos.
Lo primero sera realizar el ejercicio usando Gini como metodo para calculart la impureza.
La segunda parte la calcularemos usando la entropia. 
Por último utilizaremos la libreria tree para repetir estos cálculos aunque solo podremos utilizar Gini para calcular la impuraza.
En ambos casos mostraremos los árboles obtenidos con las librerías adecuadas.

\begin{center}
@
<<rpart3_gini, fig=TRUE>>=
muestra3 = data.frame(read.table("datos3.txt"))
clas_3gini=rpart(TipoVehiculo~., data=muestra3,method="class",minsplit=1)
rpart.plot(clas_3gini)
@
\end{center}

Ahora utilizaemos la entropia. 
Para ello añadimos el parametro parms=list(split="information").
Por defecto esta se clasifica por Gini.

\begin{center}
@
<<rpart3_entropia, fig=TRUE>>=
clas_3entropia=rpart(TipoVehiculo~., data=muestra3,method="class",minsplit=1,parms=list(split="information"))
rpart.plot(clas_3entropia)
@
\end{center}

Vemos que el árbol obtenido es distinto.
La primera diferencia que se observa, en el caso del arbol Gini su profundidad es de 3 mientras que con Entropia es de 2. 
Es preferible por tanto la clasificación utilizando entropia pues el tiempo de búsqueda en el árbol será menor.

Ahora realizamremos los mismo cálculos con tree.

\begin{center}
@
<<tree3, fig=TRUE>>=
clas_3tree=tree(TipoVehiculo~.,data=muestra3,mincut=1,minsize=2) 
draw.tree(clas_3tree)
@
\end{center}

El arbol generado es igual que el obtenido con rpart con aplicando gini.
Ya que por defecto tree implementa el gini.
En esta libreria no esta implentado el calculo por entropia.



\end{document}